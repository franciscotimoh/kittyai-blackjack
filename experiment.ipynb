{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2275e28f-5c79-4ec9-804f-01dbd690c392",
   "metadata": {},
   "source": [
    "# Solving Blackjack with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b42506-a898-4701-955d-11bc2c2efd94",
   "metadata": {},
   "source": [
    "**Actions**:\n",
    "- stand\n",
    "- hit\n",
    "- bet big\n",
    "- bet small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236f0d50-432e-450c-a767-36b12347c9d4",
   "metadata": {},
   "source": [
    "## Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca4e084-3fd3-431c-a60b-4b1d7604dd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from seaborn) (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efd4c3ab-7c9d-4e25-a127-011da0330582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tim\\miniconda3\\envs\\blackjack\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a81b3b0-ed22-4490-8d08-6e6e3b12946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym import spaces\n",
    "from gymnasium.spaces import Discrete, Tuple, Box\n",
    "from gymnasium.envs.toy_text.blackjack import BlackjackEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9923ff12-6d69-4771-bdf6-f090ae440c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBlackjackEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        # Action space: 0 = small bet, 1 = big bet, 2 = hit, 3 = stand\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Observation space: player's hand total, dealer's showing card, usable ace, balance\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(32),  # Player's hand total (0-31 to handle busts)\n",
    "            spaces.Discrete(11),  # Dealer's showing card (1-10)\n",
    "            spaces.Discrete(2),   # Usable ace (0 or 1)\n",
    "            spaces.Discrete(1001) # Balance (0 to 1000)\n",
    "        ))\n",
    "\n",
    "        # Heavy penalty for illegal actions\n",
    "        self.illegal_action_penalty = -100\n",
    "\n",
    "        # Initialize environment state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the environment to start a new episode.\"\"\"\n",
    "        self.balance = 500          # Starting balance\n",
    "        self.phase = \"bet\"          # Start with betting phase\n",
    "        self.current_bet = 0        # Current bet\n",
    "        self.player_hand = []       # Player's hand\n",
    "        self.dealer_hand = []       # Dealer's hand\n",
    "        self.usable_ace_player = False\n",
    "        self.done = False\n",
    "        return self._deal_initial_hands()\n",
    "\n",
    "    def _deal_initial_hands(self):\n",
    "        \"\"\"Deal initial hands to player and dealer.\"\"\"\n",
    "        self.player_hand = [self._draw_card(), self._draw_card()]\n",
    "        self.dealer_hand = [self._draw_card(), self._draw_card()]\n",
    "        self.usable_ace_player = 1 in self.player_hand and sum(self.player_hand) + 10 <= 21\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "        if self.phase == \"bet\":\n",
    "            if action not in [0, 1]:  # Invalid action during betting phase\n",
    "                return self._get_obs(), self.illegal_action_penalty, False, {}\n",
    "            return self._bet_phase(action)\n",
    "        \n",
    "        elif self.phase == \"play\":\n",
    "            if action not in [2, 3]:  # Invalid action during gameplay phase\n",
    "                return self._get_obs(), self.illegal_action_penalty, False, {}\n",
    "            return self._play_phase(action)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Invalid game phase.\")\n",
    "\n",
    "    def _bet_phase(self, action):\n",
    "        \"\"\"Handle betting actions.\"\"\"\n",
    "        # Reset hands for a new round\n",
    "        self.player_hand = []\n",
    "        self.dealer_hand = []\n",
    "        self.usable_ace_player = False\n",
    "        \n",
    "        if action == 0:  # Small bet\n",
    "            self.current_bet = 50\n",
    "        elif action == 1:  # Big bet\n",
    "            self.current_bet = 100\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action during betting phase.\")\n",
    "        \n",
    "        # Deduct the bet amount\n",
    "        self.balance -= self.current_bet\n",
    "        \n",
    "        # Deal new cards to player and dealer\n",
    "        self._deal_initial_hands()\n",
    "\n",
    "        # Transition to gameplay phase\n",
    "        self.phase = \"play\"\n",
    "        return self._get_obs(), 0, False, {}\n",
    "\n",
    "    def _play_phase(self, action):\n",
    "        \"\"\"Handle gameplay actions.\"\"\"\n",
    "        if action == 2:  # Hit\n",
    "            self.player_hand.append(self._draw_card())\n",
    "            self.usable_ace_player = 1 in self.player_hand and sum(self.player_hand) + 10 <= 21\n",
    "            if self._hand_value(self.player_hand) > 21:  # Player busts\n",
    "                reward = -self.current_bet\n",
    "                self.phase = \"bet\"\n",
    "                return self._get_obs(), reward, self.balance <= 0, {}\n",
    "            return self._get_obs(), 0, False, {}\n",
    "        \n",
    "        elif action == 3:  # Stand\n",
    "            # Dealer plays\n",
    "            while self._hand_value(self.dealer_hand) < 17:\n",
    "                self.dealer_hand.append(self._draw_card())\n",
    "            \n",
    "            # Determine outcome\n",
    "            player_value = self._hand_value(self.player_hand)\n",
    "            dealer_value = self._hand_value(self.dealer_hand)\n",
    "\n",
    "            if dealer_value > 21 or player_value > dealer_value:\n",
    "                reward = self.current_bet\n",
    "                self.balance += 2 * self.current_bet  # Add winnings to balance\n",
    "            elif player_value == dealer_value:\n",
    "                reward = 0\n",
    "            else:\n",
    "                reward = -self.current_bet\n",
    "            \n",
    "            self.phase = \"bet\"\n",
    "            return self._get_obs(), reward, self.balance <= 0, {}\n",
    "\n",
    "    def _draw_card(self):\n",
    "        \"\"\"Draw a card from the deck (values 1-10).\"\"\"\n",
    "        return np.random.randint(1, 11)\n",
    "\n",
    "    def _hand_value(self, hand):\n",
    "        \"\"\"Calculate the value of a hand.\"\"\"\n",
    "        value = sum(hand)\n",
    "        if 1 in hand and value + 10 <= 21:\n",
    "            return value + 10  # Use ace as 11\n",
    "        return value\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"Return the current observation.\"\"\"\n",
    "        return (\n",
    "            self._hand_value(self.player_hand),       # Player's hand total\n",
    "            self.dealer_hand[0],                     # Dealer's showing card\n",
    "            int(self.usable_ace_player),             # Usable ace\n",
    "            self.balance                              # Balance\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1853b2-52f7-4b65-82ff-cc6ef681ea64",
   "metadata": {},
   "source": [
    "## Observing our Custom Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "307aba9a-cdbb-4af5-a108-d1d6a0efc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomBlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e1b539a-0abf-418e-86c3-68c1f04999bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Observation: (10, 4, 0, 500)\n",
      "Action: 2, Observation: (10, 4, 0, 500), Reward: -100, Done: False\n",
      "Action: 1, Observation: (7, 8, 0, 400), Reward: 0, Done: False\n",
      "Action: 3, Observation: (7, 8, 0, 400), Reward: -100, Done: False\n",
      "Action: 0, Observation: (14, 4, 0, 350), Reward: 0, Done: False\n",
      "Action: 0, Observation: (14, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 1, Observation: (14, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 1, Observation: (14, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 2, Observation: (19, 4, 0, 350), Reward: 0, Done: False\n",
      "Action: 1, Observation: (19, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 0, Observation: (19, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 2, Observation: (25, 4, 0, 350), Reward: -50, Done: False\n",
      "Action: 0, Observation: (14, 10, 0, 300), Reward: 0, Done: False\n",
      "Action: 0, Observation: (14, 10, 0, 300), Reward: -100, Done: False\n",
      "Action: 2, Observation: (21, 10, 0, 300), Reward: 0, Done: False\n",
      "Action: 1, Observation: (21, 10, 0, 300), Reward: -100, Done: False\n",
      "Action: 0, Observation: (21, 10, 0, 300), Reward: -100, Done: False\n",
      "Action: 0, Observation: (21, 10, 0, 300), Reward: -100, Done: False\n",
      "Action: 1, Observation: (21, 10, 0, 300), Reward: -100, Done: False\n",
      "Action: 0, Observation: (21, 10, 0, 300), Reward: -100, Done: False\n",
      "Action: 3, Observation: (21, 10, 0, 400), Reward: 50, Done: False\n",
      "Action: 0, Observation: (7, 4, 0, 350), Reward: 0, Done: False\n",
      "Action: 3, Observation: (7, 4, 0, 350), Reward: -50, Done: False\n",
      "Action: 3, Observation: (7, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 3, Observation: (7, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 3, Observation: (7, 4, 0, 350), Reward: -100, Done: False\n",
      "Action: 0, Observation: (10, 5, 0, 300), Reward: 0, Done: False\n",
      "Action: 0, Observation: (10, 5, 0, 300), Reward: -100, Done: False\n",
      "Action: 2, Observation: (20, 5, 0, 300), Reward: 0, Done: False\n",
      "Action: 3, Observation: (20, 5, 0, 400), Reward: 50, Done: False\n",
      "Action: 1, Observation: (13, 4, 1, 300), Reward: 0, Done: False\n",
      "Action: 2, Observation: (17, 4, 1, 300), Reward: 0, Done: False\n",
      "Action: 3, Observation: (17, 4, 1, 300), Reward: -100, Done: False\n",
      "Action: 1, Observation: (11, 8, 0, 200), Reward: 0, Done: False\n",
      "Action: 0, Observation: (11, 8, 0, 200), Reward: -100, Done: False\n",
      "Action: 2, Observation: (18, 8, 0, 200), Reward: 0, Done: False\n",
      "Action: 0, Observation: (18, 8, 0, 200), Reward: -100, Done: False\n",
      "Action: 0, Observation: (18, 8, 0, 200), Reward: -100, Done: False\n",
      "Action: 3, Observation: (18, 8, 0, 400), Reward: 100, Done: False\n",
      "Action: 2, Observation: (18, 8, 0, 400), Reward: -100, Done: False\n",
      "Action: 3, Observation: (18, 8, 0, 400), Reward: -100, Done: False\n",
      "Action: 1, Observation: (18, 5, 1, 300), Reward: 0, Done: False\n",
      "Action: 0, Observation: (18, 5, 1, 300), Reward: -100, Done: False\n",
      "Action: 0, Observation: (18, 5, 1, 300), Reward: -100, Done: False\n",
      "Action: 0, Observation: (18, 5, 1, 300), Reward: -100, Done: False\n",
      "Action: 1, Observation: (18, 5, 1, 300), Reward: -100, Done: False\n",
      "Action: 1, Observation: (18, 5, 1, 300), Reward: -100, Done: False\n",
      "Action: 3, Observation: (18, 5, 1, 500), Reward: 100, Done: False\n",
      "Action: 1, Observation: (5, 6, 0, 400), Reward: 0, Done: False\n",
      "Action: 2, Observation: (10, 6, 0, 400), Reward: 0, Done: False\n",
      "Action: 3, Observation: (10, 6, 0, 600), Reward: 100, Done: False\n",
      "Action: 0, Observation: (10, 8, 0, 550), Reward: 0, Done: False\n",
      "Action: 1, Observation: (10, 8, 0, 550), Reward: -100, Done: False\n",
      "Action: 2, Observation: (14, 8, 0, 550), Reward: 0, Done: False\n",
      "Action: 1, Observation: (14, 8, 0, 550), Reward: -100, Done: False\n",
      "Action: 0, Observation: (14, 8, 0, 550), Reward: -100, Done: False\n",
      "Action: 3, Observation: (14, 8, 0, 550), Reward: -50, Done: False\n",
      "Action: 3, Observation: (14, 8, 0, 550), Reward: -100, Done: False\n",
      "Action: 1, Observation: (19, 1, 0, 450), Reward: 0, Done: False\n",
      "Action: 3, Observation: (19, 1, 0, 650), Reward: 100, Done: False\n",
      "Action: 3, Observation: (19, 1, 0, 650), Reward: -100, Done: False\n",
      "Action: 2, Observation: (19, 1, 0, 650), Reward: -100, Done: False\n",
      "Action: 1, Observation: (12, 4, 0, 550), Reward: 0, Done: False\n",
      "Action: 0, Observation: (12, 4, 0, 550), Reward: -100, Done: False\n",
      "Action: 3, Observation: (12, 4, 0, 550), Reward: -100, Done: False\n",
      "Action: 0, Observation: (11, 8, 0, 500), Reward: 0, Done: False\n",
      "Action: 3, Observation: (11, 8, 0, 500), Reward: -50, Done: False\n",
      "Action: 3, Observation: (11, 8, 0, 500), Reward: -100, Done: False\n",
      "Action: 0, Observation: (17, 7, 1, 450), Reward: 0, Done: False\n",
      "Action: 0, Observation: (17, 7, 1, 450), Reward: -100, Done: False\n",
      "Action: 3, Observation: (17, 7, 1, 450), Reward: 0, Done: False\n",
      "Action: 2, Observation: (17, 7, 1, 450), Reward: -100, Done: False\n",
      "Action: 1, Observation: (9, 6, 0, 350), Reward: 0, Done: False\n",
      "Action: 1, Observation: (9, 6, 0, 350), Reward: -100, Done: False\n",
      "Action: 3, Observation: (9, 6, 0, 350), Reward: -100, Done: False\n",
      "Action: 2, Observation: (9, 6, 0, 350), Reward: -100, Done: False\n",
      "Action: 1, Observation: (14, 9, 0, 250), Reward: 0, Done: False\n",
      "Action: 1, Observation: (14, 9, 0, 250), Reward: -100, Done: False\n",
      "Action: 2, Observation: (24, 9, 0, 250), Reward: -100, Done: False\n",
      "Action: 2, Observation: (24, 9, 0, 250), Reward: -100, Done: False\n",
      "Action: 1, Observation: (9, 3, 0, 150), Reward: 0, Done: False\n",
      "Action: 0, Observation: (9, 3, 0, 150), Reward: -100, Done: False\n",
      "Action: 1, Observation: (9, 3, 0, 150), Reward: -100, Done: False\n",
      "Action: 0, Observation: (9, 3, 0, 150), Reward: -100, Done: False\n",
      "Action: 2, Observation: (16, 3, 0, 150), Reward: 0, Done: False\n",
      "Action: 0, Observation: (16, 3, 0, 150), Reward: -100, Done: False\n",
      "Action: 2, Observation: (18, 3, 0, 150), Reward: 0, Done: False\n",
      "Action: 3, Observation: (18, 3, 0, 350), Reward: 100, Done: False\n",
      "Action: 0, Observation: (10, 5, 0, 300), Reward: 0, Done: False\n",
      "Action: 2, Observation: (19, 5, 0, 300), Reward: 0, Done: False\n",
      "Action: 0, Observation: (19, 5, 0, 300), Reward: -100, Done: False\n",
      "Action: 1, Observation: (19, 5, 0, 300), Reward: -100, Done: False\n",
      "Action: 3, Observation: (19, 5, 0, 300), Reward: -50, Done: False\n",
      "Action: 2, Observation: (19, 5, 0, 300), Reward: -100, Done: False\n",
      "Action: 0, Observation: (11, 3, 0, 250), Reward: 0, Done: False\n",
      "Action: 3, Observation: (11, 3, 0, 250), Reward: -50, Done: False\n",
      "Action: 3, Observation: (11, 3, 0, 250), Reward: -100, Done: False\n",
      "Action: 0, Observation: (15, 10, 0, 200), Reward: 0, Done: False\n",
      "Action: 2, Observation: (16, 10, 0, 200), Reward: 0, Done: False\n",
      "Action: 1, Observation: (16, 10, 0, 200), Reward: -100, Done: False\n",
      "Action: 0, Observation: (16, 10, 0, 200), Reward: -100, Done: False\n",
      "Action: 3, Observation: (16, 10, 0, 200), Reward: -50, Done: False\n",
      "Action: 1, Observation: (17, 7, 1, 100), Reward: 0, Done: False\n",
      "Action: 2, Observation: (17, 7, 0, 100), Reward: 0, Done: False\n",
      "Action: 1, Observation: (17, 7, 0, 100), Reward: -100, Done: False\n",
      "Action: 2, Observation: (18, 7, 0, 100), Reward: 0, Done: False\n",
      "Action: 2, Observation: (28, 7, 0, 100), Reward: -100, Done: False\n",
      "Action: 2, Observation: (28, 7, 0, 100), Reward: -100, Done: False\n",
      "Action: 2, Observation: (28, 7, 0, 100), Reward: -100, Done: False\n",
      "Action: 2, Observation: (28, 7, 0, 100), Reward: -100, Done: False\n",
      "Action: 0, Observation: (15, 9, 0, 50), Reward: 0, Done: False\n",
      "Action: 3, Observation: (15, 9, 0, 50), Reward: -50, Done: False\n",
      "Action: 3, Observation: (15, 9, 0, 50), Reward: -100, Done: False\n",
      "Action: 2, Observation: (15, 9, 0, 50), Reward: -100, Done: False\n",
      "Action: 0, Observation: (17, 1, 1, 0), Reward: 0, Done: False\n",
      "Action: 0, Observation: (17, 1, 1, 0), Reward: -100, Done: False\n",
      "Action: 3, Observation: (17, 1, 1, 0), Reward: -50, Done: True\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "obs = env.reset()\n",
    "print(f\"Initial Observation: {obs}\")\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"Action: {action}, Observation: {obs}, Reward: {reward}, Done: {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe863bc7-4780-4902-8263-de7268b6e15b",
   "metadata": {},
   "source": [
    "## Agent Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3dab7c-5204-41f6-a2bd-159013994224",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a2de0a8-732d-4c95-abfa-f6d1b96b73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c517dcf-536b-4015-995d-2d3fb7a4b892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table: Use a dictionary with default values for all (state, action) pairs\n",
    "Q = defaultdict(float)\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1         # Learning rate\n",
    "gamma = 0.99        # Discount factor\n",
    "epsilon = 0.1       # Exploration rate\n",
    "episodes = 10000    # Number of episodes to train\n",
    "max_rounds = 20     # Max rounds per episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "939ae83d-c05b-4c0d-a174-9ddc792a041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    \"\"\"\n",
    "    Convert the state into a discrete form for the Q-table.\n",
    "    \"\"\"\n",
    "    player_hand = min(state[0], 21)  # Limit hand value to 21\n",
    "    dealer_card = state[1]          # Dealer's showing card\n",
    "    usable_ace = state[2]           # 0 or 1\n",
    "    balance = min(state[3] // 100, 5)  # Group balance into ranges of 100\n",
    "    \n",
    "    return (player_hand, dealer_card, usable_ace, balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eed080dc-04d0-4580-806b-082c94af7ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_learning(env, Q, alpha, gamma, epsilon, episodes, max_rounds):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()  # Reset environment\n",
    "        state = discretize_state(state)  # Discretize initial state\n",
    "        done = False\n",
    "        rounds = 0\n",
    "\n",
    "        while not done and rounds < max_rounds:\n",
    "            # Epsilon-greedy action selection\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample()  # Explore\n",
    "            else:\n",
    "                # Exploit: Choose action with max Q-value\n",
    "                action = max(range(env.action_space.n), key=lambda a: Q[(state, a)])\n",
    "\n",
    "            # Take the action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = discretize_state(next_state)  # Discretize next state\n",
    "\n",
    "            # Q-learning update\n",
    "            best_next_action = max(range(env.action_space.n), key=lambda a: Q[(next_state, a)])\n",
    "            Q[(state, action)] += alpha * (reward + gamma * Q[(next_state, best_next_action)] - Q[(state, action)])\n",
    "\n",
    "            # Move to next state\n",
    "            state = next_state\n",
    "            rounds += 1\n",
    "\n",
    "        # Optionally, decay epsilon over time\n",
    "        epsilon = max(0.01, epsilon * 0.995)\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b06f7eae-da60-4744-9ca6-2a250a7f77a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, Q, episodes=100):\n",
    "    total_rewards = 0\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = discretize_state(state)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Always take the best action\n",
    "            action = max(range(env.action_space.n), key=lambda a: Q[(state, a)])\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = discretize_state(next_state)\n",
    "            total_rewards += reward\n",
    "\n",
    "    avg_reward = total_rewards / episodes\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c13747e3-7117-40df-8dbb-cb631505852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CustomBlackjackEnv()\n",
    "\n",
    "# Train the agent\n",
    "Q = train_q_learning(env, Q, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=10000, max_rounds=20)\n",
    "\n",
    "# Evaluate the agent\n",
    "# avg_reward = evaluate_agent(env, Q, episodes=100)\n",
    "# print(f\"Average Reward Over 100 Episodes: {avg_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60735645-04a0-4cd5-abb8-58654f0a477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
